{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b65104d-647e-4401-af28-506868e4073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Define the grid world\n",
    "GRID_SIZE = 3\n",
    "START = (0, 0)\n",
    "GOAL = (2, 2)\n",
    "OBSTACLE = (1, 1)\n",
    "\n",
    "# Define actions\n",
    "ACTIONS = [\n",
    "    (-1, 0),  # up\n",
    "    (0, 1),   # right\n",
    "    (1, 0),   # down\n",
    "    (0, -1)   # left\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76429fb3-8c25-44d5-bba0-a562747dad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
    "    return (0 <= state[0] < GRID_SIZE and \n",
    "            0 <= state[1] < GRID_SIZE and \n",
    "            state != OBSTACLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7148cac-9475-4767-96d7-6a03e74f10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state: Tuple[int, int], action: Tuple[int, int]) -> Tuple[int, int]:\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    return next_state if is_valid_state(next_state) else state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4355cf-6e0c-4f66-b297-f5e4460f9a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.3\n",
    "ALPHA = 0.3\n",
    "GAMMA = 0.99\n",
    "EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21b989d-37a6-4fc3-b9ae-03c94655e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state: Tuple[int, int], next_state: Tuple[int, int]) -> int:\n",
    "    if next_state == GOAL:\n",
    "        return 100\n",
    "    elif next_state == OBSTACLE or next_state == state:  # Penalize hitting walls or obstacle\n",
    "        return -10\n",
    "    else:\n",
    "        return -1  # Small penalty for each step to encourage shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44080160-cffe-4fe8-817f-d3f6b6cab200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state: Tuple[int, int], q_table: np.ndarray) -> Tuple[int, int]:\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(q_table[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd546cfc-23da-4c26-8a5d-092cd8c62a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table: np.ndarray, state: Tuple[int, int], action: Tuple[int, int], \n",
    "                   reward: int, next_state: Tuple[int, int]) -> None:\n",
    "    action_idx = ACTIONS.index(action)\n",
    "    q_table[state][action_idx] += ALPHA * (reward + GAMMA * np.max(q_table[next_state]) - q_table[state][action_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e80aa8-b7f1-46e2-ae07-ffce01922515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent() -> np.ndarray:\n",
    "    q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "    \n",
    "    for _ in range(EPISODES):\n",
    "        state = START\n",
    "        while state != GOAL:\n",
    "            action = choose_action(state, q_table)\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = get_reward(state, next_state)\n",
    "            update_q_table(q_table, state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "# Train the agent\n",
    "q_table = train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bafed3e1-dbad-4d32-adb9-0673fb899a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table_as_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the Q-table as a grid with all action values for each state.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nDetailed Q-table Grid:\")\n",
    "    \n",
    "    # Header\n",
    "    header = \"   |\" + \"|\".join(f\"   ({i},{j})   \" for i in range(GRID_SIZE) for j in range(GRID_SIZE)) + \"|\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for action_idx, action_symbol in enumerate(action_symbols):\n",
    "        row = f\" {action_symbol} |\"\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if (i, j) == GOAL:\n",
    "                    cell = \"   GOAL    \"\n",
    "                elif (i, j) == OBSTACLE:\n",
    "                    cell = \" OBSTACLE  \"\n",
    "                else:\n",
    "                    q_value = q_table[i, j, action_idx]\n",
    "                    cell = f\" {q_value:9.2f} \"\n",
    "                row += cell + \"|\"\n",
    "        print(row)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "def visualize_best_actions_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the best action and its Q-value for each state in a grid.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nBest Actions Grid:\")\n",
    "    header = \"-\" * (14 * GRID_SIZE + 1)\n",
    "    print(header)\n",
    "\n",
    "    for i in range(GRID_SIZE):\n",
    "        row = \"| \"\n",
    "        for j in range(GRID_SIZE):\n",
    "            if (i, j) == GOAL:\n",
    "                cell = \"   GOAL    \"\n",
    "            elif (i, j) == OBSTACLE:\n",
    "                cell = \" OBSTACLE  \"\n",
    "            else:\n",
    "                best_action_idx = np.argmax(q_table[i, j])\n",
    "                best_q_value = q_table[i, j, best_action_idx]\n",
    "                cell = f\"{action_symbols[best_action_idx]}:{best_q_value:7.2f}  \"\n",
    "            row += cell + \" | \"\n",
    "        print(row)\n",
    "        print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4cd371a-d3ff-4b90-9544-e6273dbca21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Q-table Grid:\n",
      "   |   (0,0)   |   (0,1)   |   (0,2)   |   (1,0)   |   (1,1)   |   (1,2)   |   (2,0)   |   (2,1)   |   (2,2)   |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " ^ |     83.12 |     85.06 |     87.02 |     92.12 | OBSTACLE  |     96.02 |     94.06 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " > |     94.06 |     96.02 |     87.02 |     85.06 | OBSTACLE  |     89.00 |     98.00 |    100.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " v |     94.06 |     85.06 |     98.00 |     96.02 | OBSTACLE  |    100.00 |     87.02 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " < |     83.12 |     92.12 |     94.06 |     85.06 | OBSTACLE  |     89.00 |     87.02 |     96.02 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best Actions Grid:\n",
      "-------------------------------------------\n",
      "| >:  94.06   | >:  96.02   | v:  98.00   | \n",
      "-------------------------------------------\n",
      "| v:  96.02   |  OBSTACLE   | v: 100.00   | \n",
      "-------------------------------------------\n",
      "| >:  98.00   | >: 100.00   |    GOAL     | \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Visualize the Q-table as a grid\n",
    "visualize_q_table_as_grid(q_table)\n",
    "\n",
    "# Visualize the best actions and their Q-values in a grid\n",
    "visualize_best_actions_grid(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
